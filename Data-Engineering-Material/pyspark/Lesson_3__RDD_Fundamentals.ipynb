{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c638fd2",
   "metadata": {},
   "source": [
    "# Lesson 3 - RDD Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb49237",
   "metadata": {},
   "source": [
    "Okay, let's structure Lesson 3 on RDD Fundamentals with the requested detail and professional focus.\n",
    "\n",
    "---\n",
    "\n",
    "**Technical Notes: PySpark RDD Fundamentals**\n",
    "\n",
    "**Objective:** These notes provide a comprehensive understanding of PySpark's foundational data abstraction, the Resilient Distributed Dataset (RDD). While modern PySpark often emphasizes DataFrames/Datasets for structured data, a solid grasp of RDDs is crucial for understanding Spark's core execution model, fault tolerance, and for handling unstructured data or scenarios requiring low-level control.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Introduction to Resilient Distributed Datasets (RDDs)**\n",
    "\n",
    "*   **Theory:**\n",
    "    An RDD is Spark's primary, low-level data abstraction, representing an **immutable, fault-tolerant, distributed collection of objects** that can be processed in parallel across a cluster. Let's break down these terms:\n",
    "    *   **Distributed:** Data within an RDD is partitioned (split) and distributed across multiple nodes (executors) in a Spark cluster. This enables parallel processing.\n",
    "    *   **Resilient:** RDDs achieve fault tolerance through lineage (discussed later). If a partition of data on a node is lost (e.g., due to node failure), Spark can automatically recompute that partition using the graph of transformations that created it.\n",
    "    *   **Immutable:** Once an RDD is created, it cannot be changed. Transformations on an RDD create *new* RDDs. This immutability simplifies consistency and fault tolerance.\n",
    "    *   **Dataset:** It's a collection of data items (e.g., numbers, strings, complex objects, key-value pairs).\n",
    "    *   **Lazily Evaluated:** Operations (Transformations) on RDDs are not executed immediately. Spark builds up a Directed Acyclic Graph (DAG) of computations, and execution is deferred until an Action is invoked (discussed later).\n",
    "\n",
    "    RDDs provide a low-level API offering fine-grained control over data placement and computation. While DataFrames offer significant optimization benefits for structured data via the Catalyst optimizer, RDDs remain relevant for:\n",
    "    *   Processing completely unstructured data (e.g., raw text, binary data).\n",
    "    *   Scenarios requiring precise control over physical data distribution and execution.\n",
    "    *   Understanding the fundamental execution model of Spark.\n",
    "\n",
    "*   **Key Properties:**\n",
    "\n",
    "    | Property         | Description                                                                 | Implication                                    |\n",
    "    | :--------------- | :-------------------------------------------------------------------------- | :--------------------------------------------- |\n",
    "    | **Distributed**  | Data is split into partitions, residing on different cluster nodes.        | Enables parallelism and scalability.           |\n",
    "    | **Immutable**    | RDDs cannot be altered after creation; transformations create new RDDs.     | Simplifies consistency, helps fault tolerance. |\n",
    "    | **Fault-Tolerant**| Can automatically recover lost data partitions using lineage.            | Provides resilience against node failures.     |\n",
    "    | **Lazy Evaluation**| Transformations are recorded, not executed, until an action is called.    | Allows for optimization, avoids wasted work.  |\n",
    "    | **Partitioned**  | The fundamental unit of parallelism; operations run on partitions.       | Performance depends on partition strategy.     |\n",
    "\n",
    "---\n",
    "\n",
    "**2. Creating RDDs**\n",
    "\n",
    "*   **Theory:**\n",
    "    Before performing operations, data must be loaded into an RDD. There are two primary ways to create RDDs in PySpark:\n",
    "    1.  **Parallelizing an existing Python collection:** Suitable for small datasets already present in the driver program's memory, often used for testing, prototyping, or distributing lookup tables.\n",
    "    2.  **Referencing an external dataset:** The more common method for real-world data, reading from distributed storage systems (like HDFS, S3, Azure Blob Storage) or local filesystems accessible by the cluster.\n",
    "\n",
    "*   **Example 1: Parallelizing a Python Collection**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Initialize SparkSession (standard entry point)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"RddCreationParallelize\") \\\n",
    "        .master(\"local[*]\") \\ # Run locally using all cores\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Get the underlying SparkContext (needed for RDD operations)\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Sample Python list\n",
    "    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    # Create an RDD by parallelizing the list\n",
    "    # 'sc.parallelize(collection, numSlices)'\n",
    "    # numSlices (optional): Suggested number of partitions. Spark might adjust.\n",
    "    numbers_rdd = sc.parallelize(data, 4) # Suggesting 4 partitions\n",
    "\n",
    "    # Verify the number of partitions\n",
    "    print(f\"RDD created with {numbers_rdd.getNumPartitions()} partitions.\")\n",
    "\n",
    "    # Display the first few elements (Action)\n",
    "    print(f\"First 5 elements: {numbers_rdd.take(5)}\")\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "    *   **Code Explanation:**\n",
    "        *   `from pyspark.sql import SparkSession`: Imports the necessary class for the entry point.\n",
    "        *   `spark = SparkSession.builder...getOrCreate()`: Creates or gets the SparkSession, configuring the application name and master URL. `local[*]` means run locally using as many cores as available.\n",
    "        *   `sc = spark.sparkContext`: Retrieves the `SparkContext` from the `SparkSession`, which is required for creating RDDs directly.\n",
    "        *   `data = [...]`: A standard Python list residing in the driver's memory.\n",
    "        *   `numbers_rdd = sc.parallelize(data, 4)`: This is the core RDD creation step. The `data` list is serialized, sent to the executors, and partitioned (here, we suggest 4 partitions). The result is an RDD (`numbers_rdd`) distributed across the (local) Spark \"cluster\".\n",
    "        *   `numbers_rdd.getNumPartitions()`: An RDD method to check the actual number of partitions created.\n",
    "        *   `numbers_rdd.take(5)`: An Action that retrieves the first 5 elements from the RDD to the driver.\n",
    "        *   `spark.stop()`: Releases resources associated with the SparkSession.\n",
    "\n",
    "    *   **Use Case:** Testing functions on small datasets, distributing small lookup tables to all executors.\n",
    "\n",
    "*   **Example 2: Reading from External Storage (Text File)**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    import os # For creating a dummy file\n",
    "\n",
    "    # Initialize SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"RddCreationTextFile\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Create a dummy text file for the example\n",
    "    file_path = \"sample_log.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"INFO: Process started\\n\")\n",
    "        f.write(\"WARN: Low memory detected\\n\")\n",
    "        f.write(\"INFO: Data loading complete\\n\")\n",
    "        f.write(\"ERROR: Connection refused\\n\")\n",
    "        f.write(\"INFO: Process finished\\n\")\n",
    "\n",
    "    # Create an RDD by reading the text file\n",
    "    # 'sc.textFile(path, minPartitions)'\n",
    "    # minPartitions (optional): Suggested *minimum* number of partitions.\n",
    "    # If reading from HDFS, it often defaults based on HDFS block size.\n",
    "    log_lines_rdd = sc.textFile(file_path, 2) # Suggest minimum 2 partitions\n",
    "\n",
    "    # Verify the number of partitions\n",
    "    print(f\"Log RDD created with {log_lines_rdd.getNumPartitions()} partitions.\")\n",
    "\n",
    "    # Count the number of lines (Action)\n",
    "    line_count = log_lines_rdd.count()\n",
    "    print(f\"Total lines in file: {line_count}\")\n",
    "\n",
    "    # Show the first line (Action)\n",
    "    first_line = log_lines_rdd.first()\n",
    "    print(f\"First line: {first_line}\")\n",
    "\n",
    "    # Clean up the dummy file\n",
    "    os.remove(file_path)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "    *   **Code Explanation:**\n",
    "        *   `os`, `with open(...)`: Standard Python code to create a temporary text file for demonstration. In a real scenario, `file_path` would point to HDFS (`hdfs://...`), S3 (`s3a://...`), or a path accessible by all cluster nodes.\n",
    "        *   `log_lines_rdd = sc.textFile(file_path, 2)`: Reads the specified file. Each line in the file becomes a separate string element in the resulting RDD (`log_lines_rdd`). Spark handles distributing the reading across partitions.\n",
    "        *   `log_lines_rdd.count()`: An Action that counts the total number of elements (lines) in the RDD.\n",
    "        *   `log_lines_rdd.first()`: An Action that retrieves the very first element (line) of the RDD.\n",
    "\n",
    "    *   **Use Case:** Processing log files, reading CSV/TSV files (though DataFrames are often better for structured formats), reading any line-delimited text data.\n",
    "\n",
    "---\n",
    "\n",
    "**3. RDD Operations: Transformations and Actions**\n",
    "\n",
    "*   **Theory:**\n",
    "    RDDs support two types of operations:\n",
    "    1.  **Transformations:** These operations create a *new* RDD from an existing one (due to immutability). Examples include `map`, `filter`, `flatMap`, `join`, `reduceByKey`. Transformations are **lazy** â€“ they define a step in the computation plan (DAG) but don't execute until an Action is called.\n",
    "    2.  **Actions:** These operations trigger the execution of the DAG built by transformations and return a result to the driver program or write data to an external storage system. Examples include `collect`, `count`, `first`, `take`, `saveAsTextFile`, `foreach`.\n",
    "\n",
    "    This lazy evaluation allows Spark to optimize the execution plan. For example, it can pipeline operations or push down filters closer to the data source.\n",
    "\n",
    "*   **Common Transformations:**\n",
    "\n",
    "    | Transformation | Description                                                                         | Input RDD Type | Output RDD Type |\n",
    "    | :------------- | :---------------------------------------------------------------------------------- | :------------- | :-------------- |\n",
    "    | `map(func)`    | Returns a new RDD by applying `func` to each element.                               | Any            | Any             |\n",
    "    | `filter(func)` | Returns a new RDD containing only elements for which `func` returns `True`.          | Any            | Same as input   |\n",
    "    | `flatMap(func)`| Similar to `map`, but each input item can be mapped to 0 or more output items (`func` should return a sequence). | Any            | Any             |\n",
    "    | `distinct()`   | Returns a new RDD with unique elements. (Involves a shuffle).                      | Any            | Same as input   |\n",
    "    | `union(otherRDD)`| Returns a new RDD containing all elements from both RDDs (duplicates included).    | Any            | Same as input   |\n",
    "    | `intersection(otherRDD)`| Returns a new RDD with elements present in *both* RDDs. (Involves shuffle).| Any            | Same as input   |\n",
    "    | `subtract(otherRDD)`| Returns a new RDD with elements present in the first RDD but not the second. (Involves shuffle). | Any            | Same as input   |\n",
    "    | `groupByKey()` | **(Key-Value RDDs)** Groups values for each key into a single sequence. (Often inefficient, prefer `reduceByKey` or `aggregateByKey`). | Pair (K, V)    | Pair (K, Iterable<V>) |\n",
    "    | `reduceByKey(func)`| **(Key-Value RDDs)** Merges values for each key using an associative and commutative `func`. Performs local aggregation before shuffling. | Pair (K, V)    | Pair (K, V)     |\n",
    "    | `sortByKey()`  | **(Key-Value RDDs)** Sorts a key-value RDD by key. (Involves shuffle).            | Pair (K, V)    | Pair (K, V)     |\n",
    "    | `join(otherRDD)`| **(Key-Value RDDs)** Performs an inner join between two key-value RDDs based on their keys. (Involves shuffle). | Pair (K, V)    | Pair (K, (V, W))|\n",
    "\n",
    "*   **Common Actions:**\n",
    "\n",
    "    | Action                 | Description                                                                                    | Return Value                  |\n",
    "    | :--------------------- | :--------------------------------------------------------------------------------------------- | :---------------------------- |\n",
    "    | `collect()`            | Returns all elements of the RDD as a list to the driver program. **Use with caution on large RDDs!** | `list`                        |\n",
    "    | `count()`              | Returns the number of elements in the RDD.                                                     | `int`                         |\n",
    "    | `first()`              | Returns the first element of the RDD.                                                          | Element Type                  |\n",
    "    | `take(n)`              | Returns the first `n` elements of the RDD as a list to the driver.                             | `list`                        |\n",
    "    | `takeOrdered(n, [key])`| Returns the first `n` elements ordered naturally or by a provided key function.               | `list`                        |\n",
    "    | `reduce(func)`         | Aggregates the elements of the RDD using a commutative and associative `func`.                 | Element Type                  |\n",
    "    | `foreach(func)`        | Applies `func` to each element of the RDD (typically for side effects like writing to DB).       | None (executes on executors) |\n",
    "    | `saveAsTextFile(path)` | Writes the elements of the RDD as text lines to a file/directory.                              | None (writes to storage)      |\n",
    "    | `countByKey()`         | **(Key-Value RDDs)** Counts the number of elements for each unique key. Returns a dictionary. | `dict`                        |\n",
    "\n",
    "*   **Example: Transformations and Actions in Sequence**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"TransformationsActions\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Start with our log lines RDD from the previous example\n",
    "    file_path = \"sample_log.txt\"\n",
    "    with open(file_path, \"w\") as f: f.write(\"INFO: A\\nWARN: B\\nINFO: C\\nERROR: D\\nINFO: E\") # Simplified content\n",
    "    log_lines_rdd = sc.textFile(file_path)\n",
    "    print(f\"Initial RDD: {log_lines_rdd.collect()}\") # Action 1\n",
    "\n",
    "    # Transformation 1: Filter for lines containing \"INFO\"\n",
    "    info_lines_rdd = log_lines_rdd.filter(lambda line: \"INFO\" in line)\n",
    "    # -- No computation happens here yet --\n",
    "    print(\"Applied filter transformation...\")\n",
    "\n",
    "    # Transformation 2: Map to extract the message part (assuming format \"LEVEL: Message\")\n",
    "    # Handle potential lines not matching the format gracefully\n",
    "    def extract_message(line):\n",
    "        parts = line.split(\":\", 1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1].strip() # Get the message part and remove whitespace\n",
    "        return \"\" # Return empty string if format doesn't match\n",
    "\n",
    "    messages_rdd = info_lines_rdd.map(extract_message)\n",
    "    # -- Still no computation --\n",
    "    print(\"Applied map transformation...\")\n",
    "\n",
    "    # Action 2: Collect the results of the filtered and mapped RDD\n",
    "    info_messages = messages_rdd.collect()\n",
    "    # -- Computation triggered here: textFile -> filter -> map -> collect --\n",
    "    print(f\"Extracted INFO messages: {info_messages}\")\n",
    "\n",
    "    # Action 3: Count the number of INFO messages\n",
    "    info_count = messages_rdd.count() # Re-uses the DAG defined by messages_rdd\n",
    "    # -- Computation triggered again (unless cached, see later): textFile -> filter -> map -> count --\n",
    "    print(f\"Count of INFO messages: {info_count}\")\n",
    "\n",
    "    # Example with Key-Value RDDs: Count occurrences of each log level\n",
    "    # Transformation 3: Map lines to (LogLevel, 1) pairs\n",
    "    level_pairs_rdd = log_lines_rdd.map(lambda line: (line.split(\":\", 1)[0], 1))\n",
    "    print(\"Applied map to create pairs...\")\n",
    "\n",
    "    # Transformation 4: Reduce by key to sum counts\n",
    "    level_counts_rdd = level_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    print(\"Applied reduceByKey...\")\n",
    "\n",
    "    # Action 4: Collect the level counts\n",
    "    level_counts_map = level_counts_rdd.collectAsMap() # Collects as a Python dictionary\n",
    "    # -- Computation triggered: textFile -> map (pairs) -> reduceByKey -> collectAsMap --\n",
    "    print(f\"Log Level Counts: {level_counts_map}\")\n",
    "\n",
    "    import os\n",
    "    os.remove(file_path)\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "    *   **Code Explanation:**\n",
    "        *   `log_lines_rdd.filter(lambda line: \"INFO\" in line)`: Creates `info_lines_rdd`. The `lambda` function checks each string; if it contains \"INFO\", the element is included in the new RDD. This is a transformation.\n",
    "        *   `info_lines_rdd.map(extract_message)`: Creates `messages_rdd`. The `extract_message` function is applied to each element (INFO line) in `info_lines_rdd` to transform it into just the message part. This is another transformation.\n",
    "        *   `messages_rdd.collect()`: This is the first **Action** on this lineage. Spark now executes the plan: read the file (`textFile`), filter lines (`filter`), extract messages (`map`), and finally gather all results into a Python list on the driver.\n",
    "        *   `messages_rdd.count()`: Another **Action**. It triggers the *same* computation DAG again (read -> filter -> map -> count) because RDDs are recomputed by default on each action.\n",
    "        *   `log_lines_rdd.map(lambda line: (line.split(\":\", 1)[0], 1))`: Creates `level_pairs_rdd`, transforming each log line into a key-value tuple like `('INFO', 1)`, `('WARN', 1)`.\n",
    "        *   `level_pairs_rdd.reduceByKey(lambda a, b: a + b)`: Creates `level_counts_rdd`. This efficient transformation groups elements by key (`'INFO'`, `'WARN'`, etc.) and applies the `lambda` function (`a + b`) cumulatively to the values (the `1`s) within each group, effectively summing them up. It performs partial aggregation locally on each partition before shuffling data, making it much preferred over `groupByKey().map(...)` for associative/commutative operations.\n",
    "        *   `level_counts_rdd.collectAsMap()`: An **Action** that executes the `textFile` -> `map` (pairs) -> `reduceByKey` DAG and returns the final key-value pairs as a Python dictionary to the driver.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Lazy Evaluation and Lineage**\n",
    "\n",
    "*   **Theory:**\n",
    "    As mentioned, transformations are **lazy**. Spark doesn't execute them immediately. Instead, it builds an internal representation of the dependencies between RDDs, known as the **lineage graph** or **Directed Acyclic Graph (DAG)**.\n",
    "    *   **DAG:** A graph where nodes represent RDDs, and directed edges represent the Transformations applied to create one RDD from another. It's \"acyclic\" because you cannot return to an older RDD state through transformations.\n",
    "    *   **Lazy Evaluation Benefits:**\n",
    "        1.  **Optimization:** Spark's Catalyst optimizer (more prominent with DataFrames, but applies conceptually here) can analyze the entire DAG and optimize the execution plan (e.g., combining `map` and `filter` into a single stage, rearranging operations).\n",
    "        2.  **Efficiency:** Computations are only performed when results are actually needed (by an Action), avoiding unnecessary work.\n",
    "        3.  **Fault Tolerance:** The lineage graph is the key to RDD resilience. If a partition of an RDD is lost (e.g., executor failure), Spark can trace back through the lineage graph and recompute *only the lost partition* from its parent RDD(s). It doesn't need to rerun the entire job.\n",
    "\n",
    "*   **Conceptual Example Walkthrough:**\n",
    "    Consider the `level_counts_rdd.collectAsMap()` action from the previous example:\n",
    "    1.  **Action Called:** `collectAsMap()` triggers the computation.\n",
    "    2.  **DAG Analysis:** Spark looks at the lineage of `level_counts_rdd`:\n",
    "        *   `level_counts_rdd` depends on `level_pairs_rdd` via `reduceByKey`.\n",
    "        *   `level_pairs_rdd` depends on `log_lines_rdd` via `map`.\n",
    "        *   `log_lines_rdd` depends on the external file via `textFile`.\n",
    "        The DAG looks like: `textFile` -> `map` -> `reduceByKey`.\n",
    "    3.  **Execution Planning:** Spark breaks the DAG into stages. Shuffles (like `reduceByKey` or `groupByKey`) typically mark stage boundaries. Within a stage, operations are often pipelined (executed together on a partition without saving intermediate results).\n",
    "        *   Stage 1: Read file (`textFile`) and perform the `map` to create pairs.\n",
    "        *   Stage 2: Shuffle the pairs based on key, then perform `reduceByKey` aggregation.\n",
    "    4.  **Task Scheduling:** Tasks (units of work within a stage, operating on one partition) are scheduled and sent to available executors.\n",
    "    5.  **Execution:** Executors perform the tasks for each stage. Intermediate shuffle data is written temporarily.\n",
    "    6.  **Result Collection:** Once Stage 2 completes, the final results are gathered by the driver for `collectAsMap()`.\n",
    "    7.  **Fault Scenario:** If an executor running a `reduceByKey` task fails, Spark notices the failure. It looks at the lineage and sees the task needs data from Stage 1 (the `map` output partition). It finds another executor to re-run the specific `map` task for the lost partition (or reads shuffle data if available) and then re-runs the failed `reduceByKey` task.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Caching and Persistence**\n",
    "\n",
    "*   **Theory:**\n",
    "    Because RDDs are recomputed by default every time an Action is called, this can be inefficient if an RDD is used multiple times (e.g., in iterative algorithms like machine learning or during interactive analysis). **Persistence** (or **caching**) allows you to explicitly request that Spark store an RDD's contents in memory, on disk, or both, after it's computed for the first time. Subsequent actions using that RDD will then read from the cache instead of recomputing its entire lineage.\n",
    "\n",
    "    *   `rdd.cache()`: This is a shorthand for `rdd.persist(StorageLevel.MEMORY_ONLY)`. It attempts to store all partitions of the RDD in the executors' memory. If there isn't enough memory, some partitions might not be cached (and would be recomputed if needed).\n",
    "    *   `rdd.persist(storageLevel)`: Offers more fine-grained control over *how* the RDD is stored.\n",
    "\n",
    "    **Common Storage Levels (`pyspark.StorageLevel`):**\n",
    "\n",
    "    | StorageLevel            | Description                                                                       | Use Case                                                         |\n",
    "    | :---------------------- | :-------------------------------------------------------------------------------- | :--------------------------------------------------------------- |\n",
    "    | `MEMORY_ONLY`           | Store RDD as deserialized Java objects in JVM memory. Fast access.                | Default `cache()`. RDD fits comfortably in memory.               |\n",
    "    | `MEMORY_ONLY_SER`       | Store RDD as *serialized* Java objects in memory. More space-efficient, CPU-intensive. | Less memory usage than `MEMORY_ONLY`, slower access.           |\n",
    "    | `MEMORY_AND_DISK`       | Store partitions in memory. If memory is full, spill excess partitions to disk.   | RDD is large, but frequent access justifies memory cost.         |\n",
    "    | `MEMORY_AND_DISK_SER`   | Like `MEMORY_AND_DISK`, but store serialized objects.                            | Balance between space efficiency, access speed, and robustness.  |\n",
    "    | `DISK_ONLY`             | Store partitions only on disk. Slowest access, but robust to memory pressure.       | RDD is very large, recomputation is very expensive.              |\n",
    "    | `MEMORY_ONLY_2`, `DISK_ONLY_2`, etc. | Replicates partitions on two cluster nodes.                            | Increased fault tolerance (survives one node failure without recompute). |\n",
    "\n",
    "    *   **Important Notes:**\n",
    "        *   Persistence itself is a **lazy** operation. The RDD is only actually cached the *first time* an Action is computed on it.\n",
    "        *   You must manually call `unpersist()` when you no longer need the cached RDD to free up storage resources.\n",
    "\n",
    "*   **Example: Using `cache()`**\n",
    "\n",
    "    ```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    import time\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PersistenceExample\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Create a large-ish RDD with some computation\n",
    "    initial_rdd = sc.parallelize(range(1, 10000001), 8) # 10M numbers, 8 partitions\n",
    "\n",
    "    # Define a somewhat costly transformation\n",
    "    def complex_computation(x):\n",
    "        # Simulate some work\n",
    "        y = x * x\n",
    "        time.sleep(0.000001) # Tiny sleep to simulate CPU time\n",
    "        return y\n",
    "\n",
    "    computed_rdd = initial_rdd.map(complex_computation)\n",
    "\n",
    "    # --- Scenario 1: Without Caching ---\n",
    "    start_time = time.time()\n",
    "    count1 = computed_rdd.count() # Action 1: Triggers computation\n",
    "    duration1 = time.time() - start_time\n",
    "    print(f\"Scenario 1 - First count: {count1}, Duration: {duration1:.2f}s\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    count2 = computed_rdd.count() # Action 2: Triggers re-computation\n",
    "    duration2 = time.time() - start_time\n",
    "    print(f\"Scenario 1 - Second count: {count2}, Duration: {duration2:.2f}s (Similar to first)\")\n",
    "\n",
    "\n",
    "    # --- Scenario 2: With Caching ---\n",
    "    # Persist the RDD in memory\n",
    "    computed_rdd.cache()\n",
    "    # Alternatively: computed_rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "    print(\"\\nRDD cached (or marked for caching)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    count3 = computed_rdd.count() # Action 3: Triggers computation and caching\n",
    "    duration3 = time.time() - start_time\n",
    "    print(f\"Scenario 2 - First count (computes & caches): {count3}, Duration: {duration3:.2f}s\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    count4 = computed_rdd.count() # Action 4: Should read from cache\n",
    "    duration4 = time.time() - start_time\n",
    "    # Note: The actual speedup depends heavily on computation cost vs cache read cost.\n",
    "    # For this trivial example, speedup might be small, but significant for heavy computations.\n",
    "    print(f\"Scenario 2 - Second count (reads from cache): {count4}, Duration: {duration4:.2f}s (Expected to be faster)\")\n",
    "\n",
    "    # Remember to unpersist when done\n",
    "    computed_rdd.unpersist()\n",
    "    print(\"\\nRDD unpersisted.\")\n",
    "\n",
    "    spark.stop()\n",
    "    ```\n",
    "\n",
    "    *   **Code Explanation:**\n",
    "        *   `initial_rdd`, `complex_computation`, `computed_rdd`: Setup an RDD and a `map` transformation that simulates some work.\n",
    "        *   **Scenario 1:** We call `count()` twice. Each call forces Spark to re-execute the `parallelize` and `map` operations. The durations should be roughly similar.\n",
    "        *   `computed_rdd.cache()`: Marks `computed_rdd` for persistence using the default `MEMORY_ONLY` level.\n",
    "        *   **Scenario 2:**\n",
    "            *   The *first* `count()` (Action 3) after `cache()` triggers the `parallelize` -> `map` computation. As partitions complete, Spark attempts to store them in memory. The duration will include computation *and* caching time.\n",
    "            *   The *second* `count()` (Action 4) should be significantly faster. Spark checks if the RDD partitions are in the cache. If found, it reads directly from memory, avoiding the expensive `complex_computation`.\n",
    "        *   `computed_rdd.unpersist()`: Releases the memory used by the cached partitions. Essential for resource management.\n",
    "\n",
    "    *   **Use Case:** Iterative algorithms (e.g., machine learning training loops where the training data RDD is reused), interactive querying of a processed dataset, checkpointing complex computations (though dedicated checkpointing has different characteristics).\n",
    "\n",
    "---\n",
    "\n",
    "**Summary & RDDs vs. DataFrames:**\n",
    "\n",
    "RDDs are the bedrock of Spark's processing model, providing distributed, resilient, immutable collections with lazy evaluation and lineage-based fault tolerance. Understanding transformations (lazy operations creating new RDDs) and actions (triggering computation) is fundamental. Caching/persistence is a key optimization technique for reusing RDDs.\n",
    "\n",
    "While powerful, RDDs lack the schema information and optimization potential of DataFrames/Datasets. For structured or semi-structured data, the **DataFrame API (covered in later lessons) is generally preferred** due to:\n",
    "*   **Catalyst Optimizer:** Performs sophisticated logical and physical query optimization.\n",
    "*   **Tungsten Execution Engine:** Uses off-heap memory management and code generation for significant performance gains.\n",
    "*   **Schema Information:** Enables more efficient storage and processing.\n",
    "*   **Richer API:** Provides SQL-like operations and domain-specific functions.\n",
    "\n",
    "However, understanding RDDs provides invaluable insight into Spark's internals and remains necessary for specific low-level tasks or unstructured data processing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
