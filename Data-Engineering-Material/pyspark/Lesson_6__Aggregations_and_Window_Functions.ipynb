{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f2ebcf",
   "metadata": {},
   "source": [
    "# Lesson 6 - Aggregations and Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4066fe3",
   "metadata": {},
   "source": [
    "Okay, let's craft the technical notes for Lesson 6: Aggregations and Window Functions in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Technical Notes: Lesson 6 - Aggregations and Window Functions\n",
    "\n",
    "**Objective:** This section delves into powerful data manipulation techniques within PySpark: aggregating data based on groups and performing calculations across related rows using window functions. These are fundamental operations for data summarization, analysis, and feature engineering in large datasets.\n",
    "\n",
    "### 1. Grouped Aggregations (`groupBy` and Aggregation Functions)\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Aggregations are operations that compute a single result from a set of input values. In distributed data processing frameworks like Spark, aggregations are often performed after grouping data based on one or more key columns.\n",
    "\n",
    "The `groupBy()` transformation in PySpark groups rows of a DataFrame based on the specified column(s). It doesn't compute anything immediately; instead, it returns a `GroupedData` object. This object holds the grouping information and requires an aggregation function (`agg()`, `sum()`, `count()`, `avg()`, `min()`, `max()`, etc.) to compute a final result DataFrame.\n",
    "\n",
    "**Execution Flow:**\n",
    "1.  **Shuffle Phase:** When `groupBy()` is called, Spark typically triggers a shuffle operation. Data rows with the same grouping key(s) from different partitions across the cluster are moved to the same worker node and partition. This can be I/O and network intensive.\n",
    "2.  **Aggregation Phase:** Once data is co-located, the specified aggregation function(s) are applied *within each group* on the respective worker nodes.\n",
    "3.  **Result:** A new DataFrame is created where each row represents a unique group key combination, and the columns contain the aggregated results.\n",
    "\n",
    "**Key Aggregation Functions (within `pyspark.sql.functions`):**\n",
    "*   `count(col)`: Counts the number of rows in each group (counts nulls if `col` is `*` or a literal).\n",
    "*   `countDistinct(col)`: Counts the number of distinct values in `col` for each group.\n",
    "*   `sum(col)`: Computes the sum of numeric values in `col` for each group.\n",
    "*   `avg(col)` or `mean(col)`: Computes the average of numeric values in `col` for each group.\n",
    "*   `min(col)`: Finds the minimum value in `col` for each group.\n",
    "*   `max(col)`: Finds the maximum value in `col` for each group.\n",
    "*   `agg(*exprs)`: Allows applying multiple aggregation functions simultaneously using a dictionary or function calls.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "Let's consider a dataset of product sales and calculate total sales amount, average sales amount, and the number of transactions per product category.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Boilerplate Spark Session creation (assuming a running Spark cluster/local mode)\n",
    "spark = SparkSession.builder.appName(\"AggregationsExample\").getOrCreate()\n",
    "\n",
    "# Sample Sales Data Schema\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"sales_amount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample Sales Data\n",
    "data = [\n",
    "    (1, \"Electronics\", 1200.00, \"North\"),\n",
    "    (2, \"Clothing\", 55.50, \"South\"),\n",
    "    (3, \"Electronics\", 800.50, \"North\"),\n",
    "    (4, \"Home Goods\", 250.00, \"West\"),\n",
    "    (5, \"Clothing\", 80.00, \"North\"),\n",
    "    (6, \"Electronics\", 150.75, \"South\"),\n",
    "    (7, \"Home Goods\", 499.99, \"North\"),\n",
    "    (8, \"Clothing\", 120.25, \"West\"),\n",
    "    (9, \"Electronics\", 2200.00, \"West\")\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(data, schema=schema)\n",
    "print(\"--- Original Sales Data ---\")\n",
    "sales_df.show()\n",
    "\n",
    "# Perform aggregations: Group by 'product_category'\n",
    "category_summary_df = sales_df.groupBy(\"product_category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"sales_amount\").alias(\"total_sales\"),\n",
    "        F.avg(\"sales_amount\").alias(\"average_sales\"),\n",
    "        F.count(\"*\").alias(\"transaction_count\") # Count all rows within the group\n",
    "    )\n",
    "\n",
    "print(\"--- Aggregated Sales Summary by Category ---\")\n",
    "category_summary_df.show()\n",
    "\n",
    "# Perform aggregations: Group by 'product_category' and 'region'\n",
    "region_category_summary_df = sales_df.groupBy(\"product_category\", \"region\") \\\n",
    "    .agg(\n",
    "        F.sum(\"sales_amount\").alias(\"total_sales\"),\n",
    "        F.count(\"transaction_id\").alias(\"transaction_count\") # Count non-null transaction IDs\n",
    "    )\n",
    "\n",
    "print(\"--- Aggregated Sales Summary by Category and Region ---\")\n",
    "region_category_summary_df.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **`from pyspark.sql import SparkSession, functions as F, types as T`**: Imports necessary classes and functions. `F` is a conventional alias for `pyspark.sql.functions`.\n",
    "2.  **`spark = SparkSession.builder...getOrCreate()`**: Standard way to initialize the SparkSession, the entry point for Spark functionality.\n",
    "3.  **`schema = StructType([...])`**: Defines the structure and data types for the DataFrame, ensuring data integrity.\n",
    "4.  **`data = [...]`**: Python list of tuples representing the raw data.\n",
    "5.  **`sales_df = spark.createDataFrame(...)`**: Creates the PySpark DataFrame from the sample data and schema.\n",
    "6.  **`sales_df.show()`**: Displays the initial DataFrame content.\n",
    "7.  **`sales_df.groupBy(\"product_category\")`**: Groups the DataFrame rows based on the unique values in the `product_category` column. Returns a `GroupedData` object.\n",
    "8.  **`.agg(...)`**: Applies aggregation functions to the grouped data.\n",
    "9.  **`F.sum(\"sales_amount\").alias(\"total_sales\")`**: Calculates the sum of the `sales_amount` column for each group and renames the resulting column to `total_sales`. `alias()` is crucial for clarity.\n",
    "10. **`F.avg(\"sales_amount\").alias(\"average_sales\")`**: Calculates the average sales amount per category.\n",
    "11. **`F.count(\"*\").alias(\"transaction_count\")`**: Counts the total number of rows within each group. Using `*` ensures all rows are counted, regardless of nulls in specific columns.\n",
    "12. **`category_summary_df.show()`**: Displays the result of the first aggregation.\n",
    "13. **`sales_df.groupBy(\"product_category\", \"region\")`**: Groups by a combination of two columns.\n",
    "14. **`F.count(\"transaction_id\").alias(\"transaction_count\")`**: Counts only the rows where `transaction_id` is not null within each group. If `transaction_id` could be null, this might differ from `count(\"*\")`.\n",
    "15. **`region_category_summary_df.show()`**: Displays the result of the multi-column grouping.\n",
    "16. **`spark.stop()`**: Releases the resources used by the SparkSession.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "*   Calculating total revenue, average order value, or customer count per region/store/month.\n",
    "*   Summarizing user activity metrics (e.g., average session duration, total clicks per user segment).\n",
    "*   Aggregating sensor readings (e.g., min/max/average temperature per sensor location per hour).\n",
    "\n",
    "**Performance Considerations:**\n",
    "*   **Cardinality of Grouping Keys:** Grouping by columns with very high cardinality (many unique values) can lead to a large number of small groups, potentially causing shuffle overhead and data skew (some nodes processing disproportionately large groups).\n",
    "*   **Pre-filtering:** Filter data *before* the `groupBy` operation whenever possible to reduce the amount of data being shuffled.\n",
    "*   **Combiners:** Spark automatically uses combiners within each partition *before* the shuffle to reduce the amount of data transferred, where applicable (e.g., for `sum`, `count`, `min`, `max`).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Pivot and Unpivot\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "**Pivot:** This operation transforms data from a \"long\" format to a \"wide\" format. It rotates unique values from a specific *pivot column* into multiple new columns. You need:\n",
    "1.  Column(s) to group by (defines the rows in the output).\n",
    "2.  A *pivot column* whose distinct values will become new column headers.\n",
    "3.  An aggregation function to determine the value that goes into the cells formed by the intersection of the group-by rows and the new pivot columns.\n",
    "\n",
    "**Unpivot:** This is the reverse operation of pivot, transforming data from a \"wide\" format back to a \"long\" format. It stacks multiple columns into a pair of columns: one for the original column name (or a category derived from it) and one for the value. PySpark doesn't have a direct `unpivot` function before Spark 3.4. The common approach is using the `stack` expression within `selectExpr` or `expr`. Spark 3.4 introduced a dedicated `unpivot` function.\n",
    "\n",
    "**Code Example (Pivot):**\n",
    "\n",
    "Let's pivot the sales data to show total sales for each category across different regions.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PivotExample\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"sales_amount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Electronics\", 1200.00, \"North\"), (2, \"Clothing\", 55.50, \"South\"),\n",
    "    (3, \"Electronics\", 800.50, \"North\"), (4, \"Home Goods\", 250.00, \"West\"),\n",
    "    (5, \"Clothing\", 80.00, \"North\"), (6, \"Electronics\", 150.75, \"South\"),\n",
    "    (7, \"Home Goods\", 499.99, \"North\"), (8, \"Clothing\", 120.25, \"West\"),\n",
    "    (9, \"Electronics\", 2200.00, \"West\")\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(data, schema=schema)\n",
    "print(\"--- Original Sales Data ---\")\n",
    "sales_df.show()\n",
    "\n",
    "# Pivot: Show total sales per category for each region\n",
    "# Explicitly list pivot values for better performance and predictable schema\n",
    "distinct_regions = [\"North\", \"South\", \"West\"] # Best practice to provide values\n",
    "\n",
    "pivoted_sales_df = sales_df.groupBy(\"product_category\") \\\n",
    "    .pivot(\"region\", distinct_regions) \\\n",
    "    .agg(F.sum(\"sales_amount\"))\n",
    "\n",
    "print(\"--- Pivoted Sales Data (Category vs Region Sales) ---\")\n",
    "pivoted_sales_df.show()\n",
    "\n",
    "# Handle potential nulls resulting from pivot (e.g., if a category had no sales in a region)\n",
    "pivoted_sales_filled_df = pivoted_sales_df.na.fill(0) # Fill nulls with 0\n",
    "\n",
    "print(\"--- Pivoted Sales Data (Filled Nulls) ---\")\n",
    "pivoted_sales_filled_df.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation (Pivot):**\n",
    "\n",
    "1.  **`sales_df.groupBy(\"product_category\")`**: Groups the data by the product category. These categories will form the rows of the pivoted table.\n",
    "2.  **`.pivot(\"region\", distinct_regions)`**: Specifies the `region` column as the pivot column. Its distinct values (`North`, `South`, `West`) will become new column headers. Providing the list `distinct_regions` is optional but recommended; otherwise, Spark needs an extra pass over the data to find the distinct values, and the resulting schema might vary.\n",
    "3.  **`.agg(F.sum(\"sales_amount\"))`**: Defines the aggregation to perform for each cell (intersection of `product_category` and `region`). Here, we sum the sales amounts. Only one aggregation function is typically used with `pivot`.\n",
    "4.  **`pivoted_sales_df.show()`**: Displays the pivoted DataFrame. Note that cells where no combination existed in the original data (e.g., potentially a category with zero sales in the 'South') will contain `null`.\n",
    "5.  **`pivoted_sales_df.na.fill(0)`**: Uses the DataFrameNaFunctions (`na`) to replace any `null` values (resulting from the pivot) with `0`.\n",
    "\n",
    "**Code Example (Unpivot using `stack` expression):**\n",
    "\n",
    "Let's take the pivoted data (or similar wide-format data) and unpivot it.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UnpivotExample\").getOrCreate()\n",
    "\n",
    "# Assume we have the pivoted data from the previous step (or create similar)\n",
    "data_pivoted = [\n",
    "    (\"Clothing\", 135.50, 55.50, 120.25),\n",
    "    (\"Electronics\", 2000.50, 150.75, 2200.00),\n",
    "    (\"Home Goods\", 499.99, None, 250.00) # Added None for South region\n",
    "]\n",
    "schema_pivoted = [\"product_category\", \"North\", \"South\", \"West\"]\n",
    "\n",
    "pivoted_df = spark.createDataFrame(data_pivoted, schema=schema_pivoted)\n",
    "print(\"--- Wide Format Data (Input for Unpivot) ---\")\n",
    "pivoted_df.show()\n",
    "\n",
    "# Unpivot using stack expression\n",
    "# stack(n, col1_name, col1_val, col2_name, col2_val, ...) -> produces n columns per row\n",
    "num_regions = 3 # Number of columns to unpivot (North, South, West)\n",
    "unpivot_expr = f\"stack({num_regions}, 'North', North, 'South', South, 'West', West) as (region, total_sales)\"\n",
    "\n",
    "unpivoted_df = pivoted_df.select(\"product_category\", F.expr(unpivot_expr)) \\\n",
    "    .where(F.col(\"total_sales\").isNotNull()) # Optional: remove rows where sales were null\n",
    "\n",
    "print(\"--- Unpivoted Data (Long Format) ---\")\n",
    "unpivoted_df.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation (Unpivot):**\n",
    "\n",
    "1.  **`data_pivoted`, `schema_pivoted`, `pivoted_df`**: Creates a sample DataFrame in a \"wide\" format, similar to the output of the pivot operation.\n",
    "2.  **`num_regions = 3`**: Defines how many columns are being unpivoted.\n",
    "3.  **`unpivot_expr = f\"stack(...)\" `**: Constructs the `stack` expression string.\n",
    "    *   `stack()` is a Spark SQL function that takes a number `n` and multiple pairs of (literal name, column value).\n",
    "    *   It creates `n` output columns (here specified as `(region, total_sales)`).\n",
    "    *   For each input row, it generates multiple output rows â€“ one for each (literal name, column value) pair.\n",
    "    *   `'North', North`: Takes the literal string 'North' and the value from the `North` column.\n",
    "    *   `'South', South`: Takes the literal string 'South' and the value from the `South` column.\n",
    "    *   `'West', West`: Takes the literal string 'West' and the value from the `West` column.\n",
    "    *   `as (region, total_sales)`: Names the two output columns generated by `stack`.\n",
    "4.  **`pivoted_df.select(\"product_category\", F.expr(unpivot_expr))`**: Selects the original grouping column (`product_category`) and applies the `stack` expression using `F.expr()`. This transforms the wide columns into rows.\n",
    "5.  **`.where(F.col(\"total_sales\").isNotNull())`**: Filters out rows that were created from `null` values in the original pivoted table (e.g., 'Home Goods' in 'South'). This step is optional depending on requirements.\n",
    "6.  **`unpivoted_df.show()`**: Displays the final \"long\" format DataFrame.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "*   **Pivot:** Creating summary tables for reports, transforming time-series data where dates/times become columns, feature engineering where distinct categories become binary/numeric features.\n",
    "*   **Unpivot:** Normalizing data that is already in a wide format (e.g., from spreadsheets or certain database exports), preparing data for tools or models that expect a long format.\n",
    "\n",
    "**Performance Considerations:**\n",
    "*   **Pivot:** Can be expensive, especially if the number of distinct values in the pivot column is large and not explicitly provided. It involves grouping and aggregation.\n",
    "*   **Unpivot (`stack`):** Generally efficient as it's primarily a projection operation, but generating many rows from one input row can increase data size.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Window Functions\n",
    "\n",
    "**Theory:**\n",
    "\n",
    "Window functions perform calculations across a set of table rows that are somehow related to the current row. Unlike `groupBy` aggregations, window functions do not collapse rows; they return a value for *each* row based on the \"window\" of related rows.\n",
    "\n",
    "A window is defined using the `Window` specification in PySpark (`pyspark.sql.window.Window`), which typically involves three components:\n",
    "\n",
    "1.  **Partitioning (`partitionBy(cols...)`):** Divides the rows into partitions (groups). The window function is applied independently within each partition. Similar to `groupBy`, but doesn't collapse rows. If omitted, the entire DataFrame is treated as a single partition.\n",
    "2.  **Ordering (`orderBy(cols...)`):** Specifies the order of rows *within each partition*. This is crucial for functions sensitive to order, like ranking (`rank`, `row_number`) or accessing preceding/succeeding rows (`lag`, `lead`).\n",
    "3.  **Frame Clause (`rowsBetween(start, end)`, `rangeBetween(start, end)`):** (Optional, more advanced) Defines the exact set of rows within the partition relative to the current row (the \"window frame\"). Defaults often suffice for common functions (e.g., for ranking, the default frame is usually the whole partition; for `lag`/`lead`, it's relative positioning). `Window.unboundedPreceding`, `Window.currentRow`, `Window.unboundedFollowing` are common boundary markers.\n",
    "\n",
    "**Common Window Functions (within `pyspark.sql.functions`):**\n",
    "\n",
    "*   **Ranking Functions:**\n",
    "    *   `rank()`: Assigns ranks based on ordering within a partition. Skips ranks after ties (e.g., 1, 1, 3).\n",
    "    *   `dense_rank()`: Assigns ranks without gaps (e.g., 1, 1, 2).\n",
    "    *   `row_number()`: Assigns a unique sequential number within the partition based on order, arbitrarily breaking ties.\n",
    "    *   `percent_rank()`: Calculates the percentile rank within the partition.\n",
    "    *   `ntile(n)`: Divides rows into `n` buckets (tiles) based on order.\n",
    "*   **Analytic Functions:**\n",
    "    *   `lag(col, offset=1, default=None)`: Accesses the value of `col` from a preceding row within the partition (defined by `orderBy`).\n",
    "    *   `lead(col, offset=1, default=None)`: Accesses the value of `col` from a succeeding row within the partition.\n",
    "*   **Aggregate Functions used as Window Functions:**\n",
    "    *   `sum(col)`, `avg(col)`, `min(col)`, `max(col)`, `count(col)` can also be used over a window frame (e.g., to calculate running totals or moving averages).\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "Let's rank employees by salary within each department and find the salary of the next lower-paid employee in the same department.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate()\n",
    "\n",
    "# Sample Employee Data Schema\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample Employee Data\n",
    "data = [\n",
    "    (1, \"Alice\", \"Sales\", 70000.0),\n",
    "    (2, \"Bob\", \"Sales\", 65000.0),\n",
    "    (3, \"Charlie\", \"Sales\", 70000.0),\n",
    "    (4, \"David\", \"IT\", 80000.0),\n",
    "    (5, \"Eve\", \"IT\", 95000.0),\n",
    "    (6, \"Frank\", \"IT\", 80000.0),\n",
    "    (7, \"Grace\", \"HR\", 55000.0),\n",
    "    (8, \"Heidi\", \"HR\", 60000.0)\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame(data, schema=schema)\n",
    "print(\"--- Original Employee Data ---\")\n",
    "emp_df.show()\n",
    "\n",
    "# Define the window specification: Partition by department, order by salary descending\n",
    "dept_window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "# Apply window functions\n",
    "emp_ranked_df = emp_df.withColumn(\"rank\", F.rank().over(dept_window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(dept_window_spec)) \\\n",
    "    .withColumn(\"row_num\", F.row_number().over(dept_window_spec)) \\\n",
    "    .withColumn(\"salary_next_lower\", F.lag(\"salary\", offset=1).over(dept_window_spec)) \\\n",
    "    .withColumn(\"salary_prev_higher\", F.lead(\"salary\", offset=1).over(dept_window_spec)) # lead gets next row val based on ORDER BY\n",
    "\n",
    "print(\"--- Employee Data with Window Function Results ---\")\n",
    "emp_ranked_df.orderBy(\"department\", \"rank\", \"emp_id\").show()\n",
    "\n",
    "# Example: Running total of salary within department (requires specific frame)\n",
    "dept_window_spec_running_total = Window.partitionBy(\"department\") \\\n",
    "                                        .orderBy(\"salary\") \\\n",
    "                                        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "emp_running_total_df = emp_df.withColumn(\"running_total_salary\",\n",
    "                                        F.sum(\"salary\").over(dept_window_spec_running_total))\n",
    "\n",
    "print(\"--- Employee Data with Running Total Salary ---\")\n",
    "emp_running_total_df.orderBy(\"department\", \"salary\").show()\n",
    "\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **`from pyspark.sql import Window`**: Imports the `Window` class needed to define window specifications.\n",
    "2.  **`emp_df = spark.createDataFrame(...)`**: Creates the employee DataFrame.\n",
    "3.  **`dept_window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))`**: Defines the window.\n",
    "    *   `partitionBy(\"department\")`: Calculations will happen independently for 'Sales', 'IT', and 'HR'.\n",
    "    *   `orderBy(F.desc(\"salary\"))`: Within each department, rows are ordered from highest salary to lowest. This order is crucial for `rank`, `row_number`, `lag`, `lead`.\n",
    "4.  **`.withColumn(\"rank\", F.rank().over(dept_window_spec))`**: Adds a new column 'rank'.\n",
    "    *   `F.rank()`: The window function being applied.\n",
    "    *   `.over(dept_window_spec)`: Specifies that `rank()` should operate over the defined window. Alice and Charlie (Sales, 70k) get rank 1, Bob (Sales, 65k) gets rank 3.\n",
    "5.  **`.withColumn(\"dense_rank\", F.dense_rank().over(dept_window_spec))`**: Adds 'dense_rank'. Alice/Charlie get 1, Bob gets 2 (no gap).\n",
    "6.  **`.withColumn(\"row_num\", F.row_number().over(dept_window_spec))`**: Adds 'row_num'. Assigns unique numbers (1, 2, 3...) based on the order, breaking ties arbitrarily but consistently within an execution.\n",
    "7.  **`.withColumn(\"salary_next_lower\", F.lag(\"salary\", offset=1).over(dept_window_spec))`**: Adds 'salary_next_lower'.\n",
    "    *   `F.lag(\"salary\", offset=1)`: Gets the value from the `salary` column of the *previous* row within the partition, based on the `orderBy` clause (descending salary). For the highest earner(s) in a dept, this will be `null`.\n",
    "8.  **`.withColumn(\"salary_prev_higher\", F.lead(\"salary\", offset=1).over(dept_window_spec))`**: Adds 'salary_prev_higher'.\n",
    "    *   `F.lead(\"salary\", offset=1)`: Gets the value from the `salary` column of the *next* row within the partition based on the `orderBy`. For the lowest earner(s) in a dept, this will be `null`.\n",
    "9.  **`emp_ranked_df.orderBy(...).show()`**: Displays the results, sorted for clarity.\n",
    "10. **`dept_window_spec_running_total = ...`**: Defines a new window spec specifically for a running total. Note the `orderBy(\"salary\")` (ascending) and the explicit frame clause.\n",
    "11. **`rowsBetween(Window.unboundedPreceding, Window.currentRow)`**: Defines the window frame: include all rows from the beginning of the partition up to and including the current row, based on the order.\n",
    "12. **`.withColumn(\"running_total_salary\", F.sum(\"salary\").over(dept_window_spec_running_total))`**: Calculates the cumulative sum of salaries within each department, ordered by salary.\n",
    "13. **`emp_running_total_df.orderBy(...).show()`**: Displays the running total results.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "*   Ranking items within categories (e.g., top N products per region).\n",
    "*   Calculating period-over-period changes (e.g., sales difference from the previous month using `lag`).\n",
    "*   Computing running totals or moving averages.\n",
    "*   Sessionization: Identifying user sessions by finding time gaps between events using `lag`.\n",
    "*   Filling missing values based on nearby rows within a partition.\n",
    "\n",
    "**Performance Considerations:**\n",
    "*   **Partitioning:** Window functions also involve data shuffling if `partitionBy` is used. Similar concerns about key cardinality and data skew apply as with `groupBy`.\n",
    "*   **Ordering:** Sorting within partitions (`orderBy`) adds computational cost.\n",
    "*   **Frame Clause:** Complex frame clauses (especially those involving large ranges or `unboundedPreceding`/`unboundedFollowing`) can impact memory usage on worker nodes, as more data per partition might need to be held for calculation.\n",
    "*   **Resource Intensive:** Window functions can be more resource-intensive than simple aggregations as they don't reduce the number of rows. Ensure sufficient cluster memory.\n",
    "\n",
    "---\n",
    "**End of Lesson 6 Notes**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
